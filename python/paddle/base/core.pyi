# Copyright (c) 2024 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any

class Place:
    def custom_device_id(self) -> int: ...
    def custom_device_type(self) -> str: ...
    def gpu_device_id(self) -> int: ...
    def ipu_device_id(self) -> int: ...
    def is_cpu_place(self) -> bool: ...
    def is_cuda_pinned_place(self) -> bool: ...
    def is_custom_place(self) -> bool: ...
    def is_gpu_place(self) -> bool: ...
    def is_ipu_place(self) -> bool: ...
    def is_xpu_place(self) -> bool: ...
    def set_place(self, place: Place) -> None: ...
    def xpu_device_id(self) -> int: ...

class CPUPlace:
    def is_float16_supported(self) -> bool: ...
    def is_bfloat16_supported(self) -> bool: ...

class CUDAPlace:
    def __init__(self, id: int, /) -> None: ...
    def get_device_id(self) -> int: ...
    def is_float16_supported(self) -> bool: ...
    def is_bfloat16_supported(self) -> bool: ...

class CUDAPinnedPlace: ...

class NPUPlace:
    def __init__(self, id: int, /) -> None: ...

class IPUPlace: ...

class CustomPlace:
    def __init__(self, name: str, id: int, /) -> None: ...
    def get_device_id(self) -> int: ...
    def get_device_type(self) -> str: ...

class MLUPlace:
    def __init__(self, id: int, /) -> None: ...

class XPUPlace:
    def __init__(self, id: int, /) -> None: ...
    def get_xpu_device_count(self) -> int: ...
    def set_xpu_debug_level(self, level: int) -> None: ...
    def get_xpu_device_version(self, device_id: int) -> int: ...
    def get_xpu_device_op_support_types(
        self, op_name: str, version: int
    ) -> list[str]: ...
    def get_xpu_device_op_list(self, version: int) -> dict[str, Any]: ...
    def is_float16_supported(self) -> bool: ...
    def is_bfloat16_supported(self) -> bool: ...

class Placement:
    def __init__(self) -> None: ...
    def __eq__(self, arg0: Placement) -> bool: ...
    def __hash__(self) -> int: ...
    def __ne__(self, arg0: Placement) -> bool: ...
    def is_partial(self) -> bool: ...
    def is_replicated(self) -> bool: ...
    def is_shard(self, dim: int | None = None) -> bool: ...

class Shard(Placement):
    def __init__(self, arg0: int) -> None: ...
    def get_dim(self) -> int: ...

class finfo:
    min: float
    max: float
    eps: float
    resolution: float
    smallest_normal: float
    tiny: float
    bits: int
    dtype: str

class iinfo:
    min: int
    max: int
    bits: int
    dtype: str

def is_compiled_with_cuda() -> bool: ...
def set_nan_inf_debug_path(arg0: str) -> None: ...

class ProgramDesc: ...
class _Scope: ...

class DistFleetWrapper:
    def barrier(self, arg0: int) -> None: ...
    def cache_shuffle(
        self, arg0: int, arg1: str, arg2: int, arg3: float
    ) -> None: ...
    def check_save_pre_patch_done(self) -> None: ...
    def client_flush(self) -> None: ...
    def create_client2client_connection(self) -> None: ...
    def get_cache_threshold(self, arg0: int) -> float: ...
    def get_client_info(self) -> list[int]: ...
    def init_fl_worker(self, arg0: list[str], arg1: int, arg2: str) -> None: ...
    def init_server(
        self,
        arg0: str,
        arg1: list[str],
        arg2: int,
        arg3: int,
        arg4: list[ProgramDesc],
    ) -> None: ...
    def init_worker(self, arg0: str, arg1: list[str], arg2: int) -> None: ...
    def load_model(self, arg0: str, arg1: int) -> None: ...
    def load_one_table(self, arg0: int, arg1: str, arg2: int) -> None: ...
    def load_sparse(self, arg0: str, arg1: str, arg2: int) -> None: ...
    def pull_dense_params(
        self, arg0: _Scope, arg1: int, arg2: list[str]
    ) -> None: ...
    def pull_fl_strategy(self) -> str: ...
    def push_dense_params(
        self, arg0: _Scope, arg1: int, arg2: list[str]
    ) -> None: ...
    def push_fl_client_info_sync(self, arg0: str) -> None: ...
    def recv_and_save_model(self, arg0: int, arg1: str) -> None: ...
    def revert(self) -> None: ...
    def run_server(self, arg0: str, arg1: int) -> int: ...
    def save_all_model(self, arg0: str, arg1: int) -> None: ...
    def save_cache(self, arg0: int, arg1: str, arg2: int) -> int: ...
    def save_cache_table(self, arg0: int, arg1: int, arg2: int) -> None: ...
    def save_one_model(self, arg0: int, arg1: str, arg2: int) -> None: ...
    def set_clients(self, arg0: list[int]) -> int: ...
    def set_date(self, arg0: int, arg1: str) -> None: ...
    def shrink_sparse_table(self, arg0: int, arg1: int) -> None: ...
    def sparse_table_stat(self, arg0: int) -> None: ...
    def stop_server(self) -> None: ...
    def stop_worker(self) -> None: ...
